{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Assignment 5 #\n","### Due: Friday, November 17th to be submitted via Canvas by 11:59 pm ###\n","### Total points: **65** ###"],"metadata":{"id":"_0gfAbD1BRax"}},{"cell_type":"markdown","source":["# Q1: Support Vector Machines (10 points)\n","\n","In this question, we will explore support vector machines for the Spam Base dataset from the UCI repository."],"metadata":{"id":"omOSpxMmBTQE"}},{"cell_type":"code","source":["import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import make_scorer, accuracy_score\n","from sklearn.model_selection import train_test_split, cross_val_score"],"metadata":{"id":"0GbdHYxjLiok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = 42"],"metadata":{"id":"tVj75ETFMaUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install ucimlrepo"],"metadata":{"id":"26ixmQ9qT6z3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ucimlrepo import fetch_ucirepo\n","# fetch dataset\n","spambase = fetch_ucirepo(id=94)\n","\n","# data (as pandas dataframes)\n","X = spambase.data.features\n","y = spambase.data.targets\n","print(\"Abstract:\", spambase.metadata['abstract'])\n","print(\"Number of instances:\", spambase.metadata['num_instances'])\n","print(\"Number of features:\", spambase.metadata['num_features'])\n","print(spambase.variables['name'])"],"metadata":{"id":"DlQyfxg0YuB0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.head()"],"metadata":{"id":"lmKGx3T8ZXci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = 42\n","y = y.to_numpy().squeeze()\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=seed)\n","\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"tukYZVbDZsG-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["a. (5 points) Implement the following function to train SVMs with a specified kernel type, hyper-parameter search space, and random state on the Spam Base dataset. Do hyper-parameter search over $C$ using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html), setting the number of folds to 5. After finding the best C, please use it to train the final model and return both the final model and the best C."],"metadata":{"id":"1jdODxjI41xm"}},{"cell_type":"code","source":["def search_best_svm(kernel, C_search_space, random_state):\n","    best_score = -np.inf\n","    for C in C_search_space:\n","        # Initialize an SVM classifier with the specified kernel type, C value, and random state\n","        ### START CODE ###\n","        ### END CODE ###\n","\n","        # Evaluate accuracy scores using 5-fold cross-validation scores\n","        ### START CODE ###\n","        ### END CODE ###\n","\n","        # Compute the average score and compare with the current best score to update the best C\n","        ### START CODE ###\n","        ### END CODE ###\n","        print(f\"C: {C} Avg Cross Val Score: {np.round(score, 4)}\")\n","\n","    print(f\"Best C: {best_C}\")\n","\n","    # Initialize the model using the specified kernel type, best C, and random state;\n","    # and then fit the model using training set\n","    ### START CODE ###\n","    ### END CODE ###\n","    return model, best_C"],"metadata":{"id":"K90sxpVK7mED"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["b. (3 points) Run the function you implemented above to train SVMs with the search space of $C$ being [$0.1, 1, 10, 100$], random state set to 42, with the following three popular kernels: (i) linear (ii) polynomial (iii) RBF (Gaussian). Evaluate your final models on the test set and report their accuracies."],"metadata":{"id":"IZ_MU9Fl6EIn"}},{"cell_type":"markdown","source":["c. (2 points) Train a logistic regression model using the training set. Compare its performance with that of the SVMs trained above."],"metadata":{"id":"pCxwqenL6nZ4"}},{"cell_type":"markdown","metadata":{"id":"bc9ca1b5"},"source":["# Question 2 : Ensemble Methods for Classification (25 pts)\n","\n","In this question, we will compare the performances of different ensemble methods for classification problems: [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html), [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) classifiers.\n","\n","We will look at the [GiveMeSomeCredit](https://www.kaggle.com/c/GiveMeSomeCredit) dataset for this question. The dataset is extremely large so for this question we will only consider a subset which has been provided along with the notebook for this assignment. The dataset has already been split into train and test sets.\n","\n","The task is to predict the probability that someone will experience financial distress in the next two years."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a93b913f","outputId":"789864e1-5a42-4df6-a7cd-4cea1102cde2","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1699650398287,"user_tz":360,"elapsed":6176,"user":{"displayName":"Song Wang","userId":"14539824110514115974"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-dc578b7f-3ab1-4613-95e2-300439976811\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-dc578b7f-3ab1-4613-95e2-300439976811\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving hw5_data.csv to hw5_data.csv\n"]}],"source":["# Only use this code block if you are using Google Colab.\n","# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n","from google.colab import files\n","\n","## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n","## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n","uploaded = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a2bac93"},"outputs":[],"source":["import pandas as pd\n","\n","data = pd.read_csv('hw5_data.csv')\n","data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis = 1, inplace = True)\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb34060d"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","y = data['SeriousDlqin2yrs']\n","X = data.drop(['SeriousDlqin2yrs'],axis=1)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 7)\n","\n","print('train:',X_train.shape, y_train.shape)\n","print('test:',X_test.shape, y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90d16082"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.model_selection import (train_test_split,GridSearchCV)\n","from sklearn.metrics import (accuracy_score,roc_auc_score)\n","from sklearn.ensemble import (RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier)\n","from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n","from sklearn.metrics import confusion_matrix\n","from sklearn.tree import DecisionTreeClassifier\n","from time import time\n","import xgboost\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"420aeb16"},"outputs":[],"source":["columns_list = list(X.columns)"]},{"cell_type":"markdown","metadata":{"id":"f40f9107"},"source":["a. (2.5 pts) Fit a Decision Tree Classifier with random_state = 14 for this classification problem. Report the accuracy_score and roc_auc_score on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"45de59e6"},"outputs":[],"source":["def fit_classifier(clf):\n","  # Fit the classifier on the training set\n","  ### START CODE ###\n","  ### END CODE ###\n","  return clf"]},{"cell_type":"code","source":["def evaluate_classifier(clf, X_test, y_test):\n","  # Compute the accuracy_score, and roc_auc_score on the test set\n","  ### START CODE ###\n","  y_pred = None\n","  y_pred_proba = None\n","\n","  acc_score = accuracy_score(None, None)\n","  auc_score = roc_auc_score(None, None)\n","  ### END CODE ###\n","  print(\"Accuracy_score: {}, ROC_AUC_score: {}\".format(acc_score, auc_score))"],"metadata":{"id":"yZfiYRKtHJQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9a436542"},"outputs":[],"source":["print(\"Decision Tree\")\n","# Initialize your decision tree classifier\n","### START CODE ###\n","dt_clf = None\n","### END CODE ###\n","\n","dt_clf = fit_classifier(dt_clf)\n","evaluate_classifier(dt_clf, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"6d360a99"},"source":["b. (2.5 pts) Create a [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) of 25 classifiers (i.e, n_estimators=25) with random_state=14. Please use Decision Tree Classifier with random_state=14 as the base classifier. Report accuracy_score and roc_auc_score on the test data for this emsemble classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"239c9c3c"},"outputs":[],"source":["print(\"Bagging of Decesion Trees\")\n","# Initialize your bagging classifier\n","### START CODE ###\n","bag_clf = None\n","### END CODE ###\n","\n","bag_clf = fit_classifier(bag_clf)\n","evaluate_classifier(bag_clf, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"34225ee3"},"source":["c. (5 pts) In this question, you will fit a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model on the training data for this classification task.\n","\n","1. First, please find the best parameters (including *n_estimators*, *max_features* and *criterion*) using [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Report the optimal parameters obtained by GridSearch.\n","2. Fit a model using the best parameters, and report the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) and [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) on test data."]},{"cell_type":"code","source":["def grid_search_for_classifier(clf, param_grid, X_train, y_train):\n","  # Grid search\n","  grid_search = GridSearchCV(clf, param_grid=param_grid)\n","\n","  # Conduct grid search using the training set (1 line of code only)\n","  ### START CODE ###\n","  ### END CODE ###\n","  print(grid_search.best_params_)\n","\n","  # Set the best paramters for your clf (1 line of code only)\n","  ### START CODE ###\n","  ### END CODE ###\n","  return clf"],"metadata":{"id":"R9E7sMGt2goM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_and_evaluate_classifier(clf, X_train, y_train, X_test, y_test):\n","  t0 = time()\n","  # Fit your classifier on the training set\n","  ### START CODE ###\n","  ### END CODE ###\n","  print(\"training time\", round(time()-t0, 3), \"s\")\n","\n","  t0 = time()\n","  y_pred = clf.predict(X_test)\n","  print(\"predict time\", round(time()-t0, 3), \"s\")\n","\n","  print(\"Confusion matrix: \")\n","  # Print the confusion matrix computed from the test set (1 line of code only)\n","  ### START CODE ###\n","  ### END CODE ###\n","\n","\n","  ### START CODE ###\n","  y_pred_proba = None\n","  acc_score = accuracy_score(None, None)\n","  auc_score = roc_auc_score(None, None)\n","  ### END CODE ###\n","\n","  print(\"Accuracy: {}, AUC_ROC: {}\".format(acc_score, auc_score))\n","  return clf"],"metadata":{"id":"4UyA2_rB2gG9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bfb9542"},"outputs":[],"source":["param_grid = {\"n_estimators\": [1, 10, 50, 100],\n","              \"max_features\": [1, 5, 10, \"auto\"],\n","              \"criterion\": ['gini','entropy'],\n","              \"random_state\": [17]}\n","\n","# Initialize your random forest classifier\n","### START CODE ###\n","rf_clf = None\n","### END CODE ###\n","rf_clf = grid_search_for_classifier(rf_clf, param_grid, X_train, y_train)\n","train_and_evaluate_classifier(rf_clf, X_train, y_train, X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"314c313b"},"source":["d. (10 pts) This time, let us use [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) for the same task. For AdaBoost and XGBoost, please respectively find the best parameters (including *n_estimators, learning_rate*); fit your model using the best parameters, and report the confusion matrix and roc_auc_score on test data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3222ef25"},"outputs":[],"source":["param_grid = {\"n_estimators\": [10, 100],\n","          \"learning_rate\": [0.01, 0.1, 0.5],\n","          \"random_state\": [17]\n","          }"]},{"cell_type":"code","source":["# Initialize your AdaBoost classifier\n","### START CODE ###\n","ab_clf = None\n","### END CODE ###\n","ab_clf = grid_search_for_classifier(ab_clf, param_grid, X_train, y_train)\n","train_and_evaluate_classifier(ab_clf, X_train, y_train, X_test, y_test)"],"metadata":{"id":"bp4Ekjpy1cGw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize your XGBoost classifier\n","### START CODE ###\n","xgb_clf = None\n","### END CODE ###\n","xgb_clf = grid_search_for_classifier(xgb_clf, param_grid, X_train, y_train)\n","train_and_evaluate_classifier(xgb_clf, X_train, y_train, X_test, y_test)"],"metadata":{"id":"Hv0p1mZh2CqA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f6bc87cd"},"source":["f. (5 pts) Compare the performance of decision tree from part a) with the ensemble methods. Briefly explain which of the three ensemble methods performed better and why?"]},{"cell_type":"markdown","source":["# Q3: CatBoost (10 points)"],"metadata":{"id":"jZXTR1tM7yBp"}},{"cell_type":"markdown","source":["In this question you will learn about a boosting algorithm known as **CatBoost**. Please go through the two videos specified below to get a better understanding of the CatBoost algorithm and answer the questions that follow.\n","\n","[Part-1](https://www.youtube.com/watch?v=KXOTSkPL2X4&ab_channel=StatQuestwithJoshStarmer)\n","[Part - 2](https://www.youtube.com/watch?v=3Bg2XRFOTzg&t=242s&ab_channel=StatQuestwithJoshStarmer)\n","\n","\n","\n","a. **(5 points)** Briefly explain Ordered Target Encoding. What challenge does it try to address?\n","\n","b. **(5 points)** Briefly describe the main advantages and disadvantages of CatBoost as compared to XGBoost."],"metadata":{"id":"3zcExque_yso"}},{"cell_type":"markdown","source":["# Q4: Convolutional Neural Network (20 points)\n","In this question, we will continue our exercise on the SVHN classification task from the previous homework, but this time we will be using Convolutional Neural Networks."],"metadata":{"id":"FTFRT23o76A2"}},{"cell_type":"code","source":["import numpy as np\n","import random\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import torchvision\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"],"metadata":{"id":"lW0odgzmXUU_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed = 42\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","np.random.seed(seed)\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)"],"metadata":{"id":"Dr0KdJ2LrCCV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["transform=torchvision.transforms.Compose([\n","        torchvision.transforms.ToTensor(),\n","        torchvision.transforms.Normalize((0.4376821, 0.4437697, 0.47280442), (0.19803012, 0.20101562, 0.19703614))\n","        ])\n","\n","train_dataset = torchvision.datasets.SVHN(root='.', split='train', transform=transform, download=True)\n","test_dataset = torchvision.datasets.SVHN(root='.', split='test', transform=transform, download=True)"],"metadata":{"id":"oBCdEj-_0tkS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_num = int(len(train_dataset) * 0.8)\n","val_num = len(train_dataset) - train_num\n","# Randomly split the training dataset into training dataset and validation dataset\n","train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_num, val_num])\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)"],"metadata":{"id":"iLn4YKK90vjo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["a. (10 points) Build a convolutional neural network with the following sequential configuration. If not specified, please use the default setting of torch.nn.Conv2d. The output of the convolution layers will be fed into a fully-connected MLP. Then train the model with Adam optimizer (lr=1e-3) for 10 epochs. You should be able to achieve test accuracy of over 85%.\n","\n","\n","\n","> Layer 1\n","*   2d convolution (# input channel=3, # output channel=16, kernel size=3, padding=1)\n","*   2d batch normalization\n","*   Relu activation\n","\n","> Pool 1\n","*   2d max pooling (kernel size=2)\n","\n","> Layer 2\n","*   2d convolution (# output channel=16, kernel size=3, padding=1)\n","*   2d batch normalization\n","*   Relu activation\n","\n","> Pool 2\n","*   2d max pooling (kernel size=2)\n","\n","> Layer 3\n","*   2d convolution (# output channel=32, kernel size=3, padding=1)\n","*   2d batch normalization\n","*   Relu activation\n","\n","> Pool 3\n","*   2d max pooling (kernel size=2)\n","\n","References:\n","\n","*   https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n","*   https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html\n","*   https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n","*   https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n","\n","\n","\n","\n"],"metadata":{"id":"RFUXsbsqjoCw"}},{"cell_type":"code","source":["class CNN(nn.Module):\n","    def __init__(self, pool=True):\n","        super(CNN, self).__init__()\n","        self.pool = pool\n","\n","        # Create convolutional layers\n","        ### START CODE ###\n","        self.layer1 = nn.Sequential()\n","\n","        self.pool1 = None\n","\n","        self.layer2 = nn.Sequential()\n","\n","        self.pool2 = None\n","\n","        self.layer3 = nn.Sequential()\n","\n","        self.pool3 = None\n","        ### END CODE ###\n","\n","        # Create fully connected layers (nn.Linear)\n","        if self.pool:\n","            self.mlp1 = nn.Linear(32*4*4, 50)\n","        else:\n","            self.mlp1 = nn.Linear(32*32*32, 50)\n","\n","        self.mlp2 = nn.Linear(50, 50)\n","        self.mlp3 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        if self.pool:\n","            x = self.pool1(x)\n","\n","        x = self.layer2(x)\n","        if self.pool:\n","            x = self.pool2(x)\n","\n","        x = self.layer3(x)\n","        if self.pool:\n","            x = self.pool3(x)\n","        x = x.reshape(x.shape[0], -1)\n","\n","        x = F.relu(self.mlp1(x))\n","        x = F.relu(self.mlp2(x))\n","        x = self.mlp3(x)\n","\n","        return x"],"metadata":{"id":"M8OcO88V01Rr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, loader, optimizer):\n","    model.train()\n","    total_loss = 0\n","    total_num = 0\n","    for data, target in tqdm(loader):\n","        out = model(data)\n","        # Calculate loss based on model output and target\n","        loss = F.nll_loss(F.log_softmax(out, dim=1), target)\n","\n","        # Use the optimizer to perform backpropagation\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        batch_size = len(target)\n","        total_loss += loss.item() * batch_size\n","        total_num += batch_size\n","    avg_loss = total_loss / total_num\n","    return avg_loss\n","\n","@torch.no_grad()\n","def eval(model, loader):\n","    model.eval()\n","    total_loss = 0\n","    total_correct = 0\n","    total_num = 0\n","    for data, target in tqdm(loader):\n","        out = model(data)\n","        # Calculate loss based on model output and target\n","        loss = F.nll_loss(F.log_softmax(out, dim=1), target)\n","\n","        # Get model's prediction\n","        pred = torch.argmax(out, dim=1)\n","\n","        # Count number of correct predictions\n","        correct = accuracy_score(target, pred, normalize=False)\n","\n","        total_correct += correct\n","        batch_size = len(target)\n","        total_loss += loss.item() * batch_size\n","        total_num += batch_size\n","    avg_loss = total_loss / total_num\n","    acc = total_correct / total_num\n","    return avg_loss, acc"],"metadata":{"id":"Ygrxekov097w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model1 = CNN(pool=True)\n","optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)\n","best_acc = -np.inf\n","epochs = 10\n","for e in range(1, epochs + 1):\n","    train_loss = train(model1, train_loader, optimizer)\n","    val_loss, val_acc = eval(model1, val_loader)\n","    if val_acc > best_acc:\n","        best_acc = val_acc\n","        best_model1 = model1\n","    print(f\"Epoch: {e} Train Loss: {train_loss} Val Loss: {val_loss} Val Acc: {val_acc}\")"],"metadata":{"id":"aET_5IqXDsC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, test_acc = eval(best_model1, test_loader)\n","print(f\"Test accuracy: {np.round(test_acc, 3)}\")"],"metadata":{"id":"A7Za_DdlDwJX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["b. (5 points) Use torch-summary to print a summary of the model. The number of parameters should be less than the one of the MLP we trained in the previous homework. Why does it have less number of parameters but have higher accuracy?\n","\n","Reference\n","*   https://pypi.org/project/torch-summary/\n"],"metadata":{"id":"xLAnNsjsnRpK"}},{"cell_type":"code","source":["!pip install torch-summary"],"metadata":{"id":"tkuRsoNxR8dX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["c. (5 points) Train another CNN with the pool option set to False. What are the differences in terms of accuracy or computation caused by disabling max pooling? What are the effects of pooling operations in CNNs? (This might take some time. Watch a TV show while you're waiting for the results..)"],"metadata":{"id":"8eb6rNHsobVb"}},{"cell_type":"code","source":["model2 = CNN(pool=False)\n","optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n","best_acc = -np.inf\n","epochs = 10\n","for e in range(1, epochs + 1):\n","    train_loss = train(model2, train_loader, optimizer)\n","    val_loss, val_acc = eval(model2, val_loader)\n","    if val_acc > best_acc:\n","        best_acc = val_acc\n","        best_model2 = model2\n","    print(f\"Epoch: {e} Train Loss: {train_loss} Val Loss: {val_loss} Val Acc: {val_acc}\")"],"metadata":{"id":"1m-9yG9VAbgE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["_, test_acc = eval(best_model2, test_loader)\n","print(test_acc)"],"metadata":{"id":"13gIhnYVAbgQ"},"execution_count":null,"outputs":[]}]}